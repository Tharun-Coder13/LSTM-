# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xyLCuQHQXUux-UqNpiwjZxB-pqOOVLh
"""

#!/usr/bin/env python3
"""
attention_time_series_forecast.py

Production-style script that:
- generates a synthetic multivariate non-stationary time series (3 features)
- builds a VAR baseline, a plain LSTM baseline, and an Attention-LSTM seq2seq model
- tunes hyperparameters for the LSTM models using Optuna
- trains and evaluates models (MAE, RMSE, MAPE)
- plots forecasts vs ground truth and visualizes attention weights

Requirements:
pip install numpy pandas matplotlib scikit-learn tensorflow optuna statsmodels tqdm

Run:
python attention_time_series_forecast.py
"""

import os
import math
import random
from typing import Tuple, Dict, Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
import statsmodels.api as sm
from statsmodels.tsa.api import VAR
import optuna
from tqdm import tqdm

# ---------------------------
# Reproducibility
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

# ---------------------------
# Config
RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)

# Hyperparams for quick run â€” change for full experiments
N_SAMPLES = 5000  # total timesteps
TRAIN_RATIO = 0.7
VAL_RATIO = 0.15
TEST_RATIO = 0.15

INPUT_SEQ_LEN = 60   # how many past timesteps to use
OUTPUT_SEQ_LEN = 12  # how many future timesteps to predict
BATCH_SIZE = 64
EPOCHS = 50
N_TRIALS = 12  # Optuna trials for demonstration; increase for robust tuning

# ---------------------------
# Utilities: metrics
def rmse(y_true, y_pred):
    return np.sqrt(np.mean((y_true - y_pred)**2))

def mape(y_true, y_pred):
    denom = np.where(np.abs(y_true) < 1e-6, 1e-6, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0

# ---------------------------
# 1) Data generation
def generate_synthetic_multivariate(n_samples: int, noise_scale=0.2) -> pd.DataFrame:
    """
    Creates 3 correlated time series with trend, seasonality, and noise.
    Features: X1 (base), X2 (~lagged & scaled), X3 (nonlinear transform + exogenous shock)
    """
    t = np.arange(n_samples)
    # Trend
    trend = 0.001 * t
    # Seasonal components (two frequencies)
    seasonal1 = 1.0 * np.sin(2 * np.pi * t / 50)        # short season
    seasonal2 = 0.5 * np.sin(2 * np.pi * t / 200)       # longer season
    noise = np.random.normal(scale=noise_scale, size=n_samples)

    x1 = 2.0 + trend + seasonal1 + seasonal2 + noise

    # x2 correlates with x1 but with lag and different amplitude + extra noise
    lag = 3
    x2 = 0.6 * np.roll(x1, lag) + 0.2 * np.sin(2 * np.pi * t / 30) + \
         0.05 * np.random.normal(size=n_samples)
    # x3 mixes x1 and x2 nonlinearly + occasional shocks
    x3 = 0.3 * x1**1.1 + 0.2 * x2 + 0.1 * np.sin(2 * np.pi * t / 80)
    shock_positions = np.random.choice(np.arange(200, n_samples-200), size=6, replace=False)
    for pos in shock_positions:
        x3[pos:pos+5] += np.random.uniform(2.0, 4.0)  # transient shocks

    df = pd.DataFrame({"x1": x1, "x2": x2, "x3": x3})
    # Introduce small missing values randomly to ensure model handles them (we'll forward fill)
    mask = np.random.rand(*df.shape) < 0.001
    df = df.mask(mask).ffill().bfill()
    return df

# ---------------------------
# 2) Prepare sliding windows
def make_windows(df: pd.DataFrame, input_len: int, output_len: int
                 ) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
    """
    Scales data and returns arrays: (X: [N, T_in, F], Y: [N, T_out, F])
    """
    scaler = StandardScaler()
    data = scaler.fit_transform(df.values)
    X_list, Y_list = [], []
    n = len(data)
    for i in range(n - input_len - output_len + 1):
        X_list.append(data[i:i+input_len])
        Y_list.append(data[i+input_len:i+input_len+output_len])
    X = np.stack(X_list)
    Y = np.stack(Y_list)
    return X, Y, scaler

# ---------------------------
# 3) Baseline: VAR model (multivariate classical baseline)
def var_forecast(train_df: pd.DataFrame, test_df: pd.DataFrame, steps: int) -> np.ndarray:
    """
    Fit VAR on train_df and forecast `steps` ahead on test start.
    Returns forecast for the first test position only (you can roll-forward to create multi-step forecast).
    For proper test set forecasting, we will iteratively forecast across the test horizon.
    """
    # Fit VAR on train
    model = VAR(train_df)
    # select order with AIC up to maxlags
    maxlags = 8
    try:
        sel = model.select_order(maxlags)
        p = int(np.nanargmin([sel.aic, sel.bic, sel.hqic])) if hasattr(sel, 'aic') else sel.selected_orders.get('aic', 1)
    except Exception:
        p = 2
    results = model.fit(p)
    # iterative forecasting across test horizon
    history = train_df.values.tolist()
    forecasts = []
    for _ in range(len(test_df)):
        pred = results.forecast(y=history[-p:], steps=1)  # one-step ahead
        forecasts.append(pred[0])
        history.append(test_df.values[len(forecasts)-1])  # use actual test value to mimic one-step forecasting
    return np.array(forecasts)  # shape: (n_test, n_features)

# ---------------------------
# 4) Keras models: Plain LSTM seq2seq and Attention-LSTM seq2seq
def build_plain_lstm(seq_in, seq_out, n_features, lstm_units=64, dropout=0.1, lr=1e-3):
    """
    Encoder-Decoder LSTM (without explicit attention). Predicts sequences.
    """
    # Encoder
    encoder_inputs = Input(shape=(seq_in, n_features), name="encoder_inputs")
    encoder_mask = layers.Masking(mask_value=0.0)(encoder_inputs)
    encoder_lstm = layers.LSTM(lstm_units, return_state=True, name="encoder_lstm")
    encoder_outputs, state_h, state_c = encoder_lstm(encoder_mask)
    encoder_states = [state_h, state_c]

    # Decoder (we'll use a RepeatVector + LSTM to predict full sequence)
    decoder = layers.RepeatVector(seq_out)(encoder_outputs)
    decoder = layers.LSTM(lstm_units, return_sequences=True)(decoder)
    decoder = layers.TimeDistributed(layers.Dense(n_features))(decoder)

    model = Model(encoder_inputs, decoder)
    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')
    return model

class BahdanauAttention(layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, enc_outputs, dec_hidden):
        # enc_outputs: (batch, T_enc, hidden)
        # dec_hidden: (batch, hidden)
        dec_hidden_time = tf.expand_dims(dec_hidden, 1)  # (batch, 1, hidden)
        score = self.V(tf.nn.tanh(self.W1(enc_outputs) + self.W2(dec_hidden_time)))  # (batch, T_enc, 1)
        attention_weights = tf.nn.softmax(score, axis=1)  # (batch, T_enc, 1)
        context_vector = attention_weights * enc_outputs  # (batch, T_enc, hidden)
        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch, hidden)
        return context_vector, tf.squeeze(attention_weights, -1)

def build_attention_lstm(seq_in, seq_out, n_features, enc_units=64, dec_units=64, lr=1e-3):
    """
    Simple encoder-decoder with Bahdanau attention. Decoder uses a loop to output sequence and capture attention weights.
    Returns (model, attention_extractor)
    """
    # Encoder
    encoder_inputs = Input(shape=(seq_in, n_features), name="enc_inputs")
    mask_layer = layers.Masking(mask_value=0.0)(encoder_inputs)
    enc_lstm = layers.Bidirectional(layers.LSTM(enc_units, return_sequences=True, return_state=True), name="enc_bilstm")
    enc_outputs, fh, fc, bh, bc = enc_lstm(mask_layer)
    # combine forward/backward states
    state_h = layers.Concatenate()([fh, bh])
    state_c = layers.Concatenate()([fc, bc])

    # Decoder initial states use encoder final states projected
    dec_input = Input(shape=(seq_out, n_features), name="dec_inputs")  # teacher forcing during training
    # We'll implement a simple recurrence using an LSTMCell
    attention = BahdanauAttention(dec_units)
    decoder_lstm_cell = layers.LSTMCell(dec_units * 2)  # because state_h concatenated
    dense_out = layers.Dense(n_features)

    # Prepare to unroll the decoder for seq_out steps
    all_outputs = []
    att_weights_all = []
    # initial states
    s0 = state_h
    c0 = state_c

    # Use TimeDistributed to take each step input (teacher forcing)
    dec_inputs_by_time = layers.Lambda(lambda x: tf.unstack(x, axis=1))(dec_input)

    s = s0
    c = c0
    for t in range(seq_out):
        x_t = dec_inputs_by_time[t]  # (batch, features)
        # compute context from encoder outputs and decoder hidden
        context, att_weights = attention(enc_outputs, s)
        # combine input and context
        x_and_context = layers.Concatenate()([x_t, context])
        # run one step of decoder LSTMCell
        output, [s, c] = decoder_lstm_cell(x_and_context, states=[s, c])
        y_t = dense_out(output)
        all_outputs.append(tf.expand_dims(y_t, axis=1))
        att_weights_all.append(tf.expand_dims(att_weights, axis=1))
    decoder_outputs = layers.Concatenate(axis=1)(all_outputs)  # (batch, seq_out, features)
    att_weights_tensor = layers.Concatenate(axis=1)(att_weights_all)  # (batch, seq_out, T_enc)

    model = Model([encoder_inputs, dec_input], decoder_outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')

    # model to extract attention: given inputs, returns att_weights_tensor
    attention_extractor = Model([encoder_inputs, dec_input], att_weights_tensor)

    return model, attention_extractor

# ---------------------------
# 5) Training helpers
def train_keras_model(model, train_ds, val_ds, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1):
    es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
    rl = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)
    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs,
                        callbacks=[es, rl], verbose=verbose)
    return history

# ---------------------------
# 6) Helper to prepare decoder inputs for teacher forcing
def prepare_decoder_inputs(Y_train):
    """
    For teacher forcing: shift-right by one step and prepend zeros (or last encoder value).
    Y_train: (N, T_out, F)
    Returns: (N, T_out, F)
    """
    zeros = np.zeros_like(Y_train[:, :1, :])
    return np.concatenate([zeros, Y_train[:, :-1, :]], axis=1)

# ---------------------------
# 7) Evaluation function for seq2seq predictions
def evaluate_forecast(y_true_scaled, y_pred_scaled, scaler, feature_names):
    """
    y_true_scaled, y_pred_scaled: shape (n_samples, T_out, n_features) in scaled space
    scaler: StandardScaler fitted on original full data (n_samples_total x n_features)
    """
    n_samples, t_out, n_features = y_true_scaled.shape
    y_true_flat = y_true_scaled.reshape(-1, n_features)
    y_pred_flat = y_pred_scaled.reshape(-1, n_features)
    # inverse transform
    y_true = scaler.inverse_transform(y_true_flat).reshape(n_samples, t_out, n_features)
    y_pred = scaler.inverse_transform(y_pred_flat).reshape(n_samples, t_out, n_features)
    metrics = {}
    mse = np.mean((y_true - y_pred)**2)
    metrics['RMSE'] = math.sqrt(mse)
    metrics['MAE'] = np.mean(np.abs(y_true - y_pred))
    metrics['MAPE'] = mape(y_true, y_pred)
    # per-feature
    for i, name in enumerate(feature_names):
        metrics[f'RMSE_{name}'] = math.sqrt(np.mean((y_true[:,:,i] - y_pred[:,:,i])**2))
        metrics[f'MAE_{name}'] = np.mean(np.abs(y_true[:,:,i] - y_pred[:,:,i]))
        metrics[f'MAPE_{name}'] = mape(y_true[:,:,i], y_pred[:,:,i])
    return metrics, y_true, y_pred

# ---------------------------
# 8) Main run
def main():
    print("1) Generating synthetic data...")
    df = generate_synthetic_multivariate(N_SAMPLES)
    feature_names = df.columns.tolist()
    print(f"Generated dataframe shape: {df.shape}, features: {feature_names}")

    # Split into train/val/test by time (no leakage)
    n = len(df)
    i_train = int(n * TRAIN_RATIO)
    i_val = i_train + int(n * VAL_RATIO)
    train_df = df.iloc[:i_train]
    val_df = df.iloc[i_train:i_val]
    test_df = df.iloc[i_val:]
    print(f"Train/Val/Test lengths: {len(train_df)}/{len(val_df)}/{len(test_df)}")

    # Make windows using entire df (scaler fit on train only ideally)
    # We'll fit scaler on train_df only to avoid leakage
    scaler = StandardScaler()
    scaler.fit(train_df.values)
    # create scaled arrays but using scaler from train only
    data_scaled = scaler.transform(df.values)
    # create windows from scaled data
    X_all, Y_all = [], []
    for i in range(len(data_scaled) - INPUT_SEQ_LEN - OUTPUT_SEQ_LEN + 1):
        X_all.append(data_scaled[i:i+INPUT_SEQ_LEN])
        Y_all.append(data_scaled[i+INPUT_SEQ_LEN:i+INPUT_SEQ_LEN+OUTPUT_SEQ_LEN])
    X_all = np.stack(X_all)
    Y_all = np.stack(Y_all)

    # Now compute indices mapping to train/val/test windows
    n_windows = X_all.shape[0]
    # window i corresponds to time index i -> input covers [i, i+INPUT_SEQ_LEN-1] and output covers [i+INPUT_SEQ_LEN, ...]
    # Determine split window indices by where the output starts
    out_start_indices = np.arange(n_windows) + INPUT_SEQ_LEN
    train_mask = out_start_indices < i_train
    val_mask = (out_start_indices >= i_train) & (out_start_indices < i_val)
    test_mask = out_start_indices >= i_val

    X_train, Y_train = X_all[train_mask], Y_all[train_mask]
    X_val, Y_val = X_all[val_mask], Y_all[val_mask]
    X_test, Y_test = X_all[test_mask], Y_all[test_mask]
    print("Windowed shapes:", X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)

    # ---------------------------
    # Baseline 1: VAR (classical)
    print("\n2) Running VAR baseline (one-step iterative forecast across test set)...")
    var_preds = var_forecast(train_df, test_df, steps=1)  # shape (n_test, n_features)
    # But Y_test expects sequences per window; to compare apples-to-apples, we'll take first output timestep of each window
    # Build y_true_single from test windows first horizon (t=0)
    y_true_first = scaler.inverse_transform(Y_test[:, 0, :])  # shape: (n_windows_test, n_features)
    # var_preds is (n_test, n_features), but may differ in length from y_true_first due to windows; align by min len
    minlen = min(var_preds.shape[0], y_true_first.shape[0])
    var_rmse = rmse(y_true_first[:minlen], var_preds[:minlen])
    var_mape = mape(y_true_first[:minlen], var_preds[:minlen])
    print(f"VAR baseline one-step RMSE: {var_rmse:.4f}, MAPE: {var_mape:.2f}%")

    # ---------------------------
    # Prepare datasets for Keras models (train/val/test)
    # For seq2seq with teacher forcing we need decoder inputs
    dec_in_train = prepare_decoder_inputs(Y_train)
    dec_in_val = prepare_decoder_inputs(Y_val)
    dec_in_test = prepare_decoder_inputs(Y_test)

    # Create tf.data datasets
    train_ds_plain = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(1024, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    val_ds_plain = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    train_ds_att = tf.data.Dataset.from_tensor_slices(((X_train, dec_in_train), Y_train)).shuffle(1024, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    val_ds_att = tf.data.Dataset.from_tensor_slices(((X_val, dec_in_val), Y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    # ---------------------------
    # Hyperparameter search with Optuna for plain LSTM and Attention LSTM
    def optuna_objective_plain(trial):
        units = trial.suggest_categorical("units", [32, 64, 128])
        lr = trial.suggest_loguniform("lr", 1e-4, 1e-2)
        dropout = trial.suggest_uniform("dropout", 0.0, 0.3)
        model = build_plain_lstm(INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, X_train.shape[-1], lstm_units=units, dropout=dropout, lr=lr)
        # quick fit for few epochs to evaluate
        history = model.fit(train_ds_plain, validation_data=val_ds_plain, epochs=6, verbose=0)
        val_loss = min(history.history['val_loss'])
        tf.keras.backend.clear_session()
        return val_loss

    def optuna_objective_att(trial):
        enc_units = trial.suggest_categorical("enc_units", [32, 64])
        dec_units = trial.suggest_categorical("dec_units", [32, 64])
        lr = trial.suggest_loguniform("lr", 1e-4, 1e-2)
        model, _ = build_attention_lstm(INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, X_train.shape[-1],
                                        enc_units=enc_units, dec_units=dec_units, lr=lr)
        history = model.fit(train_ds_att, validation_data=val_ds_att, epochs=6, verbose=0)
        val_loss = min(history.history['val_loss'])
        tf.keras.backend.clear_session()
        return val_loss

    print("\n3) Running Optuna tuning (quick trials)...")
    study_plain = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED))
    study_plain.optimize(optuna_objective_plain, n_trials=N_TRIALS, show_progress_bar=True)
    print("Plain LSTM best params:", study_plain.best_params)

    study_att = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED+1))
    study_att.optimize(optuna_objective_att, n_trials=N_TRIALS, show_progress_bar=True)
    print("Attention LSTM best params:", study_att.best_params)

    # ---------------------------
    # Build final models with best params
    print("\n4) Building final models with chosen hyperparameters...")
    # Plain LSTM
    p = study_plain.best_params
    lstm_model = build_plain_lstm(INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, X_train.shape[-1],
                                 lstm_units=int(p.get('units', 64)),
                                 dropout=float(p.get('dropout', 0.1)),
                                 lr=float(p.get('lr', 1e-3)))
    # Attention LSTM
    pa = study_att.best_params
    att_model, att_extractor = build_attention_lstm(INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, X_train.shape[-1],
                                                    enc_units=int(pa.get('enc_units', 64)),
                                                    dec_units=int(pa.get('dec_units', 64)),
                                                    lr=float(pa.get('lr', 1e-3)))

    # Train both fully
    print("Training Plain LSTM...")
    history_plain = train_keras_model(lstm_model, train_ds_plain, val_ds_plain, epochs=EPOCHS)
    print("Training Attention LSTM...")
    history_att = train_keras_model(att_model, train_ds_att, val_ds_att, epochs=EPOCHS)

    # ---------------------------
    # Forecast on test sets
    # Plain LSTM: model expects X -> outputs Y_pred (no decoder inputs)
    print("\n5) Forecasting on test windows...")
    y_pred_plain_scaled = lstm_model.predict(X_test, batch_size=128)
    # Attention LSTM: requires decoder input (teacher forcing). At inference, we use zero initial decoder inputs and feed previous predicted step (auto-regressive)
    # We'll implement simple autoregressive inference using att_extractor? No, att_extractor only returns attention, so use att_model with iterative feeding.
    def autoregressive_predict_attention(enc_inputs, seq_out, model, att_extractor, scaler):
        """
        enc_inputs: (N, T_in, F) scaled
        We'll produce outputs autoregressively: feed zeros as initial decoder input and then append predicted steps.
        """
        n = enc_inputs.shape[0]
        n_features = enc_inputs.shape[2]
        outputs = np.zeros((n, seq_out, n_features), dtype=np.float32)
        dec_input = np.zeros((n, seq_out, n_features), dtype=np.float32)
        # iterative: for t from 0..seq_out-1, call model with current dec_input, but that's expensive to call per step.
        # Simpler approach: use teacher-forcing style at inference by shifting previous predicted values into dec_input progressively and call model once each step.
        for t in range(seq_out):
            dec_in_partial = np.zeros_like(dec_input)
            dec_in_partial[:, :t, :] = outputs[:, :t, :]
            # call model to predict all timesteps; take timestep t
            pred_full = model.predict([enc_inputs, dec_in_partial], batch_size=128)
            outputs[:, t, :] = pred_full[:, t, :]
        return outputs

    y_pred_att_scaled = autoregressive_predict_attention(X_test, OUTPUT_SEQ_LEN, att_model, att_extractor, scaler)

    # ---------------------------
    # Evaluate models
    print("\n6) Evaluating models...")
    metrics_plain, y_true_plain, y_pred_plain = evaluate_forecast(Y_test, y_pred_plain_scaled, scaler, feature_names)
    metrics_att, y_true_att, y_pred_att = evaluate_forecast(Y_test, y_pred_att_scaled, scaler, feature_names)

    print("Plain LSTM metrics (aggregated):")
    print({k: (f"{v:.4f}" if isinstance(v, float) else v) for k, v in metrics_plain.items() if not k.startswith('RMSE_')})
    print("Attention LSTM metrics (aggregated):")
    print({k: (f"{v:.4f}" if isinstance(v, float) else v) for k, v in metrics_att.items() if not k.startswith('RMSE_')})

    # Print specific metrics of interest
    print(f"\nFinal: Plain LSTM RMSE: {metrics_plain['RMSE']:.4f}, MAPE: {metrics_plain['MAPE']:.2f}%")
    print(f"Final: Attention LSTM RMSE: {metrics_att['RMSE']:.4f}, MAPE: {metrics_att['MAPE']:.2f}%")
    print(f"VAR baseline one-step RMSE (earlier): {var_rmse:.4f}, MAPE: {var_mape:.2f}%")

    # ---------------------------
    # Save some plots: forecast vs actual for last test windows
    print("\n7) Plotting forecasts vs actual for first test sample and last test sample...")
    def plot_forecast_sample(y_true, y_pred, sample_idx, feature_idx=0, title_suffix=""):
        plt.figure(figsize=(8,4))
        true_series = y_true[sample_idx,:,feature_idx]
        pred_series = y_pred[sample_idx,:,feature_idx]
        plt.plot(true_series, label='True', marker='o')
        plt.plot(pred_series, label='Pred', marker='x')
        plt.title(f"Feature {feature_names[feature_idx]} forecast vs true {title_suffix}")
        plt.xlabel("Horizon")
        plt.legend()
        fname = os.path.join(RESULTS_DIR, f"forecast_sample_{title_suffix}_{feature_names[feature_idx]}_{sample_idx}.png")
        plt.tight_layout()
        plt.savefig(fname)
        plt.close()
        print("Saved", fname)

    # Save for plain and att for a couple samples and features
    for model_label, y_pred, y_true in [("plain", y_pred_plain, y_true_plain), ("att", y_pred_att, y_true_att)]:
        plot_forecast_sample(y_true, y_pred, sample_idx=0, feature_idx=0, title_suffix=f"{model_label}_first")
        plot_forecast_sample(y_true, y_pred, sample_idx=-1, feature_idx=0, title_suffix=f"{model_label}_last")

    # ---------------------------
    # Attention analysis: extract attention weights for some test samples using att_extractor
    print("\n8) Extracting and plotting attention weights for several test samples...")
    # For extraction we need decoder inputs used during inference; we used autoregressive partial predictions; we can reconstruct decoder inputs accordingly.
    # For simplicity, we'll use the first test sample and its decoder inputs (zeros then previous preds)
    sample_idx = 0
    enc_input = X_test[sample_idx:sample_idx+1]
    # Build decoder input with zeros then previously predicted steps
    dec_infer = np.zeros((1, OUTPUT_SEQ_LEN, X_test.shape[2]))
    att_weights = att_extractor.predict([np.repeat(enc_input, 1, axis=0), dec_infer])  # shape (1, T_out, T_enc)
    # Plot attention heatmap
    att_map = att_weights[0]  # (T_out, T_enc)
    plt.figure(figsize=(10,6))
    plt.imshow(att_map, aspect='auto', interpolation='nearest')
    plt.colorbar(label='Attention weight')
    plt.xlabel("Encoder time steps")
    plt.ylabel("Decoder horizon")
    plt.title("Attention weights (decoder horizon x encoder timesteps)")
    fname = os.path.join(RESULTS_DIR, "attention_weights_sample0.png")
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()
    print("Saved", fname)

    # Print a short textual analysis of what attention shows for sample 0
    max_att_per_decoder_step = att_map.argmax(axis=1)
    print("Attention peak encoder indices per decoder step (sample 0):", max_att_per_decoder_step.tolist())
    print("Interpretation hint: indices correspond to encoder timestep index (0 is oldest input in window). Peaks near higher indices mean attention to recent inputs.")

    # ---------------------------
    # Save CSV summary results
    summary = {
        "model": ["VAR_one_step", "Plain_LSTM", "Attention_LSTM"],
        "RMSE": [var_rmse, metrics_plain['RMSE'], metrics_att['RMSE']],
        "MAPE": [var_mape, metrics_plain['MAPE'], metrics_att['MAPE']]
    }
    pd.DataFrame(summary).to_csv(os.path.join(RESULTS_DIR, "model_comparison_summary.csv"), index=False)
    print("Saved model comparison summary to CSV.")

    # Print final key outputs (as requested: RMSE/MAPE for Attention model)
    print(f"\nATTENTION MODEL FINAL METRICS -> RMSE: {metrics_att['RMSE']:.4f}, MAPE: {metrics_att['MAPE']:.2f}%")